# Copyright (c) 2025 Andrey Volkov

# This work is a derivative work based on the original by Mahtab Syed.
# Original author copyright notice:
# MIT License
# Copyright (c) 2025 Mahtab Syed
# https://www.linkedin.com/in/mahtabsyed/

import os
import random
import re
import sys
import json
import unidiff
import time
from pathlib import Path
from google import genai
from patch import patch_code, fix_patch, is_unified_diff
from md_parser import find_code_blocks

# Initialize Gemini LLM
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("GEMINI_API_KEY environment variable not set")

llm_model = "gemini-2.5-flash"
nonthinking_llm_model = "gemini-2.5-flash-lite"
print(f"ðŸ“¡ Initializing Gemini LLM ({llm_model})...")
llm = genai.Client(api_key=api_key)
llm_config = genai.types.GenerateContentConfig(
    temperature=0.3,
    tools=[genai.types.Tool(code_execution=genai.types.ToolCodeExecution)]
)

llm_config_coder = genai.types.GenerateContentConfig(
    temperature=0.3,
    tools=[genai.types.Tool(code_execution=genai.types.ToolCodeExecution)],
)

llm_config_reformatter = genai.types.GenerateContentConfig(
    temperature=0,
)

llm_config_goals_check = genai.types.GenerateContentConfig(
    temperature=0.3,
    responseMimeType="text/x.enum",
    responseSchema={
        "type": "string",
        "enum": ["Yes", "No"],
    },
)

def llm_query(query, config=llm_config, model=llm_model):
    # mark start time
    start_time = time.monotonic()
    response = llm.models.generate_content(
        model=model, contents=query, config=config
    )
    end_time = time.monotonic()
    # Calculate generation time in seconds
    generation_time = end_time - start_time
    text = response.text

    return {"text": text, "full": response, "usage": response.usage_metadata, "response_time": generation_time}

def print_usage_info(metadata, time):
    print("Token Usage Info: total {}, cache {}, candidates {}, prompt {}, thoughts {}, tool_use {}".format(
        metadata.total_token_count,
        metadata.cached_content_token_count,
        metadata.candidates_token_count,
        metadata.prompt_token_count,
        metadata.thoughts_token_count,
        metadata.tool_use_prompt_token_count
    ))
    print(f"Time taken for LLM call: {time:.1f} seconds")

# --- Utility Functions ---
def load_file(filepath: str) -> str:
    with open(filepath, "r") as f:
        return f.read()

def generate_prompt(use_case: str, goals: str, previous_code: str = None, feedback: str = None) -> str:
    print("ðŸ“ Constructing prompt for code generation...")

    if previous_code:
        print("ðŸ”„ Adding previous code to the prompt for refinement.")
        script_path = "scripts/coder fix.md"
    else:
        script_path = "scripts/coder create.md"

    script = load_file(script_path)
    base_prompt = script.format_map({
        "use_case": use_case,
        "goals": goals,
        "previous_code": previous_code,
        "feedback": feedback
    })

    return base_prompt

def get_code_feedback(use_case: str, code: str, goals: str, code_output: str) -> str:
    print("ðŸ” Evaluating code against the goals...")

    script_path = "scripts/reviewer.md"
    script = load_file(script_path)
    feedback_prompt = script.format_map({
        "use_case": use_case,
        "goals": goals,
        "code": code,
        "code_output": code_output
    })
    return llm_query(feedback_prompt)["text"]

def goals_met(feedback_text: str, goals: str) -> bool:
    """
    Uses the LLM to evaluate whether the goals have been met based on the feedback text.
    Returns True or False (parsed from LLM output).
    """
    script_path = "scripts/goals check.md"
    script = load_file(script_path)
    review_prompt = script.format_map({
        "goals": goals,
        "feedback_text": feedback_text
    })  
    response = llm_query(review_prompt, config=llm_config_goals_check)["text"].strip().lower()
    print(f"ðŸŽ¯ Goals met evaluation: {response}")
    return response == "yes"

def clean_code_block(code) -> str:
    if not isinstance(code, list):
        lines = code.splitlines()
    else:
        lines = code
    if lines and lines[0].strip().startswith("```"):
        lines = lines[1:]
    if lines and lines[-1].strip() == "```":
        lines = lines[:-1]
    return "\n".join(lines)

def add_comment_header(code: str, use_case: str) -> str:
    comment = []
    comment.append(f"# This Python program implements the following use case:")
    for line in use_case.strip().splitlines():
        comment.append(f"# {line.strip()}")
    comment.append(f"# Generated by AI Code Generation Agent using {llm_model}\n")
    return "\n".join(comment) + "\n" + code

def to_snake_case(text: str) -> str:
    text = re.sub(r"[^a-zA-Z0-9 ]", "", text)
    return re.sub(r"\s+", "_", text.strip().lower())

def create_short_filename(use_case: str) -> str:
    summary_prompt = (
        f"Summarize the following use case into a single lowercase word or phrase, "
        f"no more than 10 characters, suitable for a Python filename:\n\n{use_case}"
    )
    raw_summary = llm_query(summary_prompt)["text"].strip()
    short_name = re.sub(r"[^a-zA-Z0-9_]", "", raw_summary.replace(" ", "_").lower())[:10]
    random_suffix = str(random.randint(1000, 9999))
    return f"{short_name}_{random_suffix}"

def save_to_file(filename: str, code: str) -> str:
    filepath = Path.cwd() / "solutions" / filename
    with open(filepath, "w") as f:
        f.write(code)
    print(f"âœ… Saved to: {filepath}")
    return str(filepath)

# --- Main Agent Function ---
def run_code_agent(use_case: str, goals: str, max_iterations: int = 5) -> str:
    
    print("\nðŸŽ¯ Use Case:")
    print(use_case)
    print("ðŸŽ¯ Goals:")
    print(goals)
    
    filename = create_short_filename(use_case)
    print(f" ðŸ” Base name is {filename} for this run")

    previous_code = None
    feedback = None
    for i in range(max_iterations):
        print(f"\n=== ðŸ” Iteration {i + 1} of {max_iterations} ===")
        prompt = generate_prompt(use_case, goals, previous_code, feedback)
        
        print("ðŸš§ Generating code...")
        code_response = llm_query(prompt, config=llm_config_coder)
        print_usage_info(code_response["usage"], code_response["response_time"])
        
        print("ðŸ§¾ Processing LLM output...")
        try:
            # Save JSON response for debugging
            with open(f"solutions/{filename}_debug_response_v{i+1}.json", "w") as f:
                f.write(code_response["full"].model_dump_json(indent=2))
            with open(f"solutions/{filename}_debug_response_text_v{i+1}.json", "w") as f:
                f.write(code_response["text"])

            print("running reformatter...")
            reformat_script = load_file("scripts/reformatter.md")
            reformat_prompt = reformat_script.format_map({
                "output": code_response["text"],
            })
            
            reformatter_response = llm_query(reformat_prompt, config=llm_config_reformatter, model=nonthinking_llm_model)
            print_usage_info(reformatter_response["usage"], reformatter_response["response_time"])

            with open(f"solutions/{filename}_debug_reformatter_v{i+1}.json", "w") as f:
                f.write(reformatter_response["full"].model_dump_json(indent=2))
            with open(f"solutions/{filename}_debug_reformatter_text_v{i+1}.md", "w") as f:
                f.write(reformatter_response["text"])

            text = reformatter_response["text"]
            code_blocks = find_code_blocks(text, delimiter="~~~", language="python")
            diff_blocks = find_code_blocks(text, delimiter="~~~", language="diff")
            out_blocks = find_code_blocks(text, delimiter="~~~", language="shell")

            if code_blocks:
                with open(f"solutions/{filename}_debug_reformatter_code_v{i+1}.py", "w") as f:
                    f.write(code_blocks[0])
            if diff_blocks:
                with open(f"solutions/{filename}_debug_reformatter_diff_v{i+1}.patch", "w") as f:
                    f.write(diff_blocks[0])
            if out_blocks:
                with open(f"solutions/{filename}_debug_reformatter_out_v{i+1}.txt", "w") as f:
                    f.write(out_blocks[0])

            code_output = out_blocks[0] if len(out_blocks) > 0 else ""
            if isinstance(code_output, list):
                code_output = "\n".join(code_output)

            if diff_blocks:
                patch_lines = clean_code_block(diff_blocks[0]).splitlines()
                print("ðŸ› ï¸ Detected unified diff patch. Applying patch to previous code.")
                patch_filename = f"{filename}_v{i+1}.patch"

                if previous_code is None:
                    raise ValueError("No previous code to apply patch to.")
                prev_code_lines = previous_code.splitlines()
                fix_patch(patch_lines)
                print(f"ðŸ’¾ Saving patch to file {patch_filename}")
                save_to_file(patch_filename, "\n".join(patch_lines))

                patch_code(prev_code_lines, patch_lines)
                code = '\n'.join(prev_code_lines)
            else:
                if code_blocks:
                    code = clean_code_block(code_blocks[0])
                else:
                    code = ""
        except Exception as e:
            print(f"âŒ Error processing LLM output: {e}")
            print("Restarting iteration...")
            continue

        # print("\nðŸ§¾ Generated Code:\n" + "-" * 50 + f"\n{code}\n" + "-" * 50)
        # if code_output:
        #     print("\nðŸ’» Code Execution Output:\n" + "-" * 50 + f"\n{code_output}\n" + "-" * 50)

        code_filename = f"{filename}_v{i+1}.py"
        print(f"ðŸ’¾ Saving intermediate code to file {code_filename}")
        save_to_file(code_filename, code)

        if code_output:
            code_output_filename = f"{filename}_v{i+1}_output.txt"
            print(f"ðŸ’¾ Saving intermediate code output to file {code_output_filename}")
            save_to_file(code_output_filename, code_output)

        print("\nðŸ“¤ Submitting code for feedback review...")
        feedback = get_code_feedback(use_case, code, goals, code_output)
        feedback_text = feedback.strip()
        # print("\nðŸ“¥ Feedback Received:\n" + "-" * 50 + f"\n{feedback_text}\n" + "-" * 50)

        review_filename = f"{filename}_review_v{i+1}.txt"
        print(f"ðŸ’¾ Saving review to file {review_filename}")
        save_to_file(review_filename, feedback_text)

        if goals_met(feedback_text, goals):
            print("âœ… LLM confirms goals are met. Stopping iteration.")
            break

        print("ðŸ› ï¸ Goals not fully met. Preparing for next iteration...")
        previous_code = code

    final_code = add_comment_header(code, use_case)
    code_filename = f"{filename}.py"
    print(f"ðŸ’¾ Saving final code to file {code_filename}")
    return save_to_file(code_filename, final_code)

# --- CLI Test Run ---
if __name__ == "__main__":
    print("\nðŸ§  Welcome to the AI Code Generation Agent")

    # Configuration name is the first command-line argument
    config_name = sys.argv[1] if len(sys.argv) > 1 else None

    if not config_name:
        print("Please provide a configuration name as the first argument.")
        sys.exit(1)

    if not os.path.exists(f"tasks/{config_name}/"):
        print(f"Configuration for '{config_name}' not found in 'tasks/{config_name}/'.")
        sys.exit(1)

    use_case_input = load_file(f"tasks/{config_name}/hl_spec.md")
    goals_input = load_file(f"tasks/{config_name}/ac.md")
    run_code_agent(use_case_input, goals_input, max_iterations=25)
    